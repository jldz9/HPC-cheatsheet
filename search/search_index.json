{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-hpc-world","title":"Welcome to HPC world!","text":"<p>University research HPCs typically run Slurm. However, since each HPC team often configures Slurm differently, users face learning curve every time they switch to a new system. </p> <p>This documentation is designed to get you up and running with HPC systems quickly to save your time and frustration because there are more important things in life than drowning in command hell. </p> <p>I assume you have fundimental understanding of: </p> <ul> <li>Linux</li> </ul>"},{"location":"#system-requirement","title":"System Requirement","text":"<p>No matter what you have, you just need ssh installed</p>"},{"location":"#cheat-sheet-catalog","title":"Cheat Sheet Catalog","text":"<p>SciNet:</p> <ul> <li>Ceres</li> <li>Atlas</li> </ul> <p>CSU: </p> <ul> <li>Cashew</li> <li>Rivera</li> </ul>"},{"location":"#need-help","title":"Need Help?","text":"<p>Meanwhile, if you have any question relate to the documentation or willing to contribute, please feel free to submit ,  join ,  or send me </p>"},{"location":"about/","title":"About","text":""},{"location":"about/#about","title":"About","text":""},{"location":"cheatsheet/batch_script/","title":"Example Batch Script","text":"<p>You can copy the batch script below to use <code>sbatch run.sh</code> to submit </p>"},{"location":"cheatsheet/batch_script/#csu","title":"CSU","text":""},{"location":"cheatsheet/batch_script/#cashew","title":"Cashew","text":"<p>Note</p> <p>remove <code>#SBATCH --gres=gpu:1</code> if you do not want to use GPU</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=dev \n#SBATCH --output=codetunnel_%j.out\n#SBATCH --error=codetunnel_%j.err\n#SBATCH --time=1-00:00:00\n#SBATCH --partition=all\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=32\n#SBATCH --gres=gpu:1 \n#SBATCH --mem=64G\n#SBATCH --nodes=1\n#SBATCH --mail-user=youremail@colostate.edu\n#SBATCH --mail-type=ALL\n\necho \"Job started\"\n\n##put your command here##\n\nwait\n\necho \"Job finished\"\n</code></pre>"},{"location":"cheatsheet/batch_script/#riviera","title":"Riviera","text":"<p>Note</p> <p>remove <code>#SBATCH --gres=gpu:1</code> if you do not want to use GPU</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=dev \n#SBATCH --output=codetunnel_%j.out\n#SBATCH --error=codetunnel_%j.err\n#SBATCH --time=1-00:00:00\n#SBATCH --partition=day-long-gpu\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=32\n#SBATCH --gres=gpu:1 \n#SBATCH --mem=64G\n#SBATCH --nodes=1\n#SBATCH --mail-user=youremail@colostate.edu\n#SBATCH --mail-type=ALL\n\necho \"Job started\"\n\n##put your command here##\n\nwait\n\necho \"Job finished\"\n</code></pre>"},{"location":"cheatsheet/main/","title":"Cheatsheets","text":"<p>This page will list cheat sheet for different HPC groups, you may use catalogs to find specific HPC</p>"},{"location":"cheatsheet/main/#scinet","title":"SciNet","text":""},{"location":"cheatsheet/main/#ceres","title":"Ceres","text":"<p>Login</p> Method Code/Link Note Login <code>ssh user.name@ceres.scinet.usda.gov</code> Replace the <code>user.name</code> with your username in Scinet DTN Login <code>ssh user.name@ceres-dtn.scinet.usda.gov</code> DTN refer to data transfer node GUI Open Ondemand Login tutorial and Usage <p>Data Transfer</p> <p>Note</p> <p>Example to copy a directory from your local system to the HPC </p> <p>Simply switch the order if you want transfer HPC --&gt; local</p> Method Code/Link Note Windows <code>scp -r path\\to\\target  user.name@ceres-dtn.scinet.usda.gov:/path/to/dest</code> use \" \\ \" on for windows path seperator, also if you are copying files instead of directory, remove <code>-r</code> Linux <code>rsync -avz --no-p --no-g /path/to/target &lt;user.name&gt;@ceres-dtn.scinet.usda.gov:/path/to/dest</code> MacOS <code>rsync -avz --no-p --no-g /path/to/target &lt;user.name&gt;@ceres-dtn.scinet.usda.gov:/path/to/dest</code> Use <code>--iconv utf-8-mac</code> if run into issue source GUI Globus Instruction <p>Compute </p> <p>Warning</p> <p>Do not run computational intensive job on login node, you need to allocate a computing node</p> Method Code/Link Note Allocate resource <code>salloc -n 2 -N 1 --mem 128G --cpus-per-task 8 -p short -t 01:00:00</code> Reserve <code>-n 2</code> tasks  in <code>-N 1</code> Node  with <code>--mem=128GB</code> memory under <code>-p short</code> partitions  for <code>-t 1</code> hour, Ceres gives you an interactive shell after allocated the resource Job run <code>srun salloc -n 8 -N 1 --mem=128G -p=short -t 01:00:00 your_command</code> Similiar with <code>salloc</code> but you can run your command use <code>srun</code> and specific the resource you need Batch run Batch script generator Generally like to combine multiple <code>srun</code> together , check Tutorial for more detail Check job <code>squeue -A</code> Cancel job <code>scancel jobID</code> JobID can be obtained from <code>squeue</code> <p>Software </p> Method Code/Link Note Check module availibility <code>module avail your_module</code> your_module refer to the actual module name load module <code>module load your_module</code> If there are different versions for your_module, this will load the default version, use your_module/version_number to load the version you want self install software <code>module load miniconda; conda install or pip install</code> This will load Conda package manager and allow you to self-install package, check Toturial Use container <code>module load apptainer; apptainer foo</code> HPC environment cannot allow docker because of user privilege, apptainer is a great alternative"},{"location":"cheatsheet/main/#atlas","title":"Atlas","text":"<p>Access</p> Method Code/Link Note Login <code>ssh user.name@Atlas-login.hpc.msstate.edu</code> Replace the <code>user.name</code> with your username in Scinet DTN Login <code>ssh user.name@Atlas-dtn.hpc.msstate.edu</code> DTN refer to data transfer node GUI Open Ondemand login tutorial and Usage <p>Data Transfer</p> <p>Note</p> <p>Example to copy a directory from your local system to the HPC </p> <p>Simply switch the order if you want transfer HPC --&gt; local</p> Method Code/Link Note Windows <code>scp -r path\\to\\target  user.name@Atlas-dtn.hpc.msstate.edu:/path/to/dest</code> use \" \\ \" on windows path, also if you are copying files instead of directory, remove <code>-r</code> Linux <code>rsync -avz --no-p --no-g /path/to/target &lt;user.name&gt;@Atlas-dtn.hpc.msstate.edu:/path/to/dest</code> MacOS <code>rsync -avz --no-p --no-g /path/to/target &lt;user.name&gt;@Atlas-dtn.hpc.msstate.edu:/path/to/dest</code> Use <code>--iconv utf-8-mac</code> if run into issue source GUI Globus Instruction <p>Compute </p> <p>Warning</p> <p>Do not run computational intensive job on login node, you need to allocate a computing node</p> Method Code/Link Note Allocate resource <code>salloc -n 2 -N 1 --cpus-per-task 8 --mem 128G -p short -t 01:00:00 -A your_user_group</code> Atlas does not excute interactive shell after use <code>salloc</code> and requires user group input, the user group typically is your project name, check Miscellaneous section in the cheat sheet for how to get your user group Interactive <code>srun salloc -n 1 -N 1 --mem=128G -p=short -t 01:00:00 -A your_user_group --pty bash</code> Get a shell on a compute job job run <code>srun salloc -n 1 -N 1 --mem=128G -p=short -t 01:00:00 -A your_user_group your_command</code> Similiar with <code>salloc</code> but you can run your command use <code>srun</code> and specific the resource you need Batch run Batch script generator Generally like to combine multiple <code>srun</code> together , check Tutorial for more detail Check job <code>squeue -A \"Your user group\"</code> Cancel job <code>scancel jobID</code> JobID can be obtained from <code>squeue</code> <p>Software </p> Method Code/Link Note Check module availibility <code>module avail your_module</code> your_module refer to the actual module name load module <code>module load your_module</code> If there are different versions for your_module, this will load the default version, use your_module/version_number to load the version you want self install software <code>module load miniconda; conda install or pip install</code> This will load Conda package manager and allow you to self-install package, check Toturial Use container <code>module load apptainer; apptainer foo</code> HPC environment cannot allow docker because of user privilege, apptainer is a great alternative"},{"location":"cheatsheet/main/#csu","title":"CSU","text":""},{"location":"cheatsheet/main/#cashew","title":"Cashew","text":"<p>Login</p> Method Code/Link Note Login <code>ssh user.name@cashew.engr.colostate.edu</code> Replace the <code>user.name</code> with your username in Scinet GUI Open Ondemand <p>Data Transfer</p> <p>Note</p> <p>Example to copy a directory from your local system to the HPC </p> <p>Simply switch the order if you want transfer HPC --&gt; local</p> Method Code/Link Note Windows <code>scp -r path\\to\\source  user.name@cashew.engr.colostate.edu:/path/to/dest</code> Use \" \\ \" on windows path, also if you are copying files instead of directory, remove <code>-r</code> Linux <code>rsync -avz --no-p --no-g /path/to/source user.name@cashew.engr.colostate.edu:/path/to/dest</code> MacOS <code>rsync -avz --no-p --no-g /path/to/source user.name@cashew.engr.colostate.edu:/path/to/dest</code> Use <code>--iconv utf-8-mac</code> if run into issue source GUI open Ondemand Under Files <p>Compute </p> <p>Warning</p> <p>Do not run computational intensive job on login node, you need to allocate a computing node</p> Method Code/Link Note Check HPC <code>sinfo</code> You may find availiable partition name, time limit to be useful Interactive node <code>srun -n 1 -N 1 --mem 32GB --cpus-per-task 16 --gres=gpu:1 -p all -t 01:00:00 --pty bash</code> Get an instance on compute node, <code>-n 1</code> task <code>-N 1</code> node <code>--mem 32GB</code> memory <code>--cpus-per-task 16</code> cpus <code>-gres=gpu:1</code> gpus <code>-p all</code> partition <code>-t 01:00:00</code> hour <code>--pty bash</code> bash shell job run <code>srun -n 1 -N 1 --mem 32GB --cpus-per-task 16 --gres=gpu:1 -p all -t 01:00:00 your_job_executable</code> Job will be terminated when you close the terminal, if you want background processing use <code>sbatch</code> instead Batch run Copy Batch Script use <code>sbatch your_batch_file</code> to submit batch jobs. More detail on cashew website Check job <code>squeue -u your_user_name</code> or just use <code>squeue</code> to check all running instances Cancel job <code>scancel jobID</code> JobID can be obtained from <code>squeue</code> <p>Software </p> Method Code/Link Note Check module availibility <code>module avail your_module</code> e.g. <code>module avail git</code> load module <code>module load your_module</code> To load specific version, e.g. <code>module load git/2.46.0</code> self install software <code>conda install</code> or <code>pip install</code> Cashew will load conda and pip by default, you don't need to <code>module load conda</code> Use container Container env is not availiable on Cashew HPC"},{"location":"cheatsheet/main/#riviera","title":"Riviera","text":"<p>Login</p> Method Code/Link Note Login <code>ssh user.name@riviera.colostate.edu</code> Replace the <code>user.name</code> with your username in Scinet GUI Riviera does not support OpenOnDemand <p>Data Transfer</p> <p>Note</p> <p>Example to copy a directory from your local system to the HPC </p> <p>Simply switch the order if you want transfer HPC --&gt; local</p> Method Code/Link Note Windows <code>scp -r path\\to\\source  user.name@cashew.engr.colostate.edu:/path/to/dest</code> Use \" \\ \" on windows path, if you are copying files instead of directory, remove <code>-r</code> Linux <code>rsync -avz --no-p --no-g /path/to/source user.name@cashew.engr.colostate.edu:/path/to/dest</code> MacOS <code>rsync -avz --no-p --no-g /path/to/source user.name@cashew.engr.colostate.edu:/path/to/dest</code> Use <code>--iconv utf-8-mac</code> if run into issue source GUI Riviera does not support web GUI for data transfer, User could use winSCP instead <p>Compute </p> <p>Warning</p> <p>Do not run computational intensive job on login node, you need to allocate a computing node</p> <p>Note</p> <p>Riviera HPC does not load slurm by default, run <code>module load slurm</code> first to load slurm module</p> Method Code/Link Note Check HPC <code>sinfo</code> You may find availiable partition name, time limit to be useful Interactive node <code>srun -n 1 -N 1 --mem 32GB --cpus-per-task 16 --gres=gpu:1 -p day-long-gpu -t 01:00:00 --pty bash</code> Get an instance on compute node, <code>-n 1</code> task <code>-N 1</code> node <code>--mem 32GB</code> memory <code>--cpus-per-task 16</code> cpus <code>-gres=gpu:1</code> gpus <code>-p day-long-gpu</code> partition <code>-t 01:00:00</code> hour <code>--pty bash</code> bash shell job run <code>srun -n 1 -N 1 --mem 32GB --cpus-per-task 16 --gres=gpu:1 -p day-long-gpu -t 01:00:00 your_job_executable</code> Job will be terminated when you close the terminal, if you want background processing use <code>sbatch</code> instead Batch run Copy Batch Script use <code>sbatch your_batch_file</code> to submit batch jobs. More detail on cashew website Check job <code>squeue -u your_user_name</code> or just use <code>squeue</code> to check all running instances Cancel job <code>scancel jobID</code> JobID can be obtained from <code>squeue</code> <p>Software </p> Method Code/Link Note Check module availibility <code>module avail your_module</code> e.g. <code>module avail git</code> load module <code>module load your_module</code> To load specific version, e.g. <code>module load git/2.46.0</code> self install software <code>conda install</code> or <code>pip install</code> Riviera will load conda and pip by default, you don't need to <code>module load conda</code> Use container <code>module load singularity</code> Only Singularity availiable, apptainer is not supported"}]}